{% extends "layout/subpage.html" %}
{% block article_content %}
<div>
  {{ heading("Colony Counter") }}
  {{ subheading("Product System Overview") }} 
  <div>
    {{ subsubheading("Abstract") }}
    <div>
      <p>This page outlines the overarching product system, materials needed, build and test procedures, and possible future next steps for the colony counter. The automated colony counter was built out of a need to minimize manual time in counting colonies on petri dishes and uses machine learning to achieve this automated computation. With a finetuned instance of DETR, the team realized a mAP of 0.038 with some overfitting. The physical hardware is relatively simple and consists of a Raspberry Pi to hold the camera and the machine learning model and an Arduino microcontroller to provide human/machine interfacing functionality.</p>
      
    </div>
    {{ subsubheading("Introduction") }}
    <div>
      <p>In the team's experience, counting colonies on a petri dish can take up to forty minutes per plate. Colony counting is an essential task for the growth assays the wet lab team requires. In particular, quantification of origin of replication efficacy demands quantifying growth assays through either repeated OD600 or colony count measurements, ideally as a time series. To achieve high throughput in wet lab operations, this bottleneck in measurement must be expedited as much as possible.</p>
      
      <p>Automated colony counting has been made possible through machine learning, as demonstrated by {{cite('Majchrowska_Pawlowski_Gula_Bonus_Hanas_Loch_Pawlak_Roszkowiak_Golan_Drulis-Kawa_2021')}}. This task is a specific case of the object detection machine learning problem, made famous through datasets such as COCO {{cite('Lin_Maire_Belongie_Hays_Perona_Ramanan_Doll√°r_Zitnick_2014')}}. The aforementioned resource used a ResNet-50 based object detection model {{cite('Majchrowska_Pawlowski_Gula_Bonus_Hanas_Loch_Pawlak_Roszkowiak_Golan_Drulis-Kawa_2021')}}. Our team plans on improving upon AGAR's work by using the same dataset but a higher capacity model in the form of the DETR vision transformer [ref, DETR]. In keeping with the team's focus on internet of things architectures, any product system the team creates will be modular and compatible with extensions towards wireless/remote operation as well as integration with other digital devices, enabling creators to quickly combine this system with other components and upscale.</p>
      
    </div>
    {{ subsubheading("Architecture") }}
    <div>
      <p>The combined product system consists of a Raspberry Pi 5 single board computer with an Arduino microcontroller.</p>
      
      <p><i>Process diagram</i></p>
      {{img(new_cdn("hardware/screenshot-2024-08-25-195112.png"), width="80%")}}
      Figure 1. Overall product system process diagram.
      
      <i>Raspberry Pi</i>
      <p>
        Any full Raspberry Pi board above Pi 3 will suffice, with the caveat that it possess enough computing power for machine learning inference tasks. For demonstrative purposes, a Raspberry Pi 5 will be used in this literature. The Raspberry Pi 5 houses a fine-tuned DETR model, based on {{cite('Ma_Hia_Lin_Kulkarni_Slavkova_Sanchez_Han_Gelana_2024')}}. All inference is to be done on the Raspberry Pi 5. The Raspberry Pi 5 possesses a RAM of 8GB. The camera used is the High Quality Camera with a Wide Angle Lens from the original equipment manufacturer. The Arduino microcontroller is connected from the Raspberry Pi with a USB wire. The serial communication protocol is used. The Arduino microcontroller is powered by the Raspberry Pi this way.
      </p>
      
      <i>Arduino microcontroller</i>
      <p>
        Any Arduino with the capability to communicate with the I2C communication protocol will suffice. For demonstrative purposes, an Arduino Uno Wifi Rev2 will be used in this discussion. The Arduino will house all human/machine interaction components so as to shield the Raspberry Pi from electrical disturbances. The Arduino will be equipped with two buttons and an LCD display to provide human/machine interaction capability.
      </p>
      
      <i>Machine learning model</i>
      <p>
        The machine learning model used is DETR <ref>. Code from the MedSAM project from the Bowang lab at University of Toronto Mississauga was adapted by Chloe Nguyen Chau, a dry lab team member assisting the hardware team.
      </p>
      
    </div>
    {{ subsubheading("Bill of Materials") }}
    <div>
      <table class="entrepreneurship">
        <tbody>
          <tr>
            <td><b>Full name and description</b></td>
            <td><b>Quantity</b></td>
            <td><b>Remarks</b></td>
          </tr>
          <tr>
            <td>Arduino Uno Wifi Rev2</td>
            <td>1</td>
            <td>Any other Arduino will do, though it must support I2C. The <code>SDA</code> and <code>SCK</code> pins (see wiring diagram) are located differently depending on the board.</td>
          </tr>
          <tr>
            <td>16x2 LCD</td>
            <td>1</td>
            <td>Any LCD will work as long as it is compatible with the Hitachi HD44780 driver.</td>
          </tr>
          <tr>
            <td>Push buttons</td>
            <td>2</td>
            <td>To change the state of the Arduino (finite state machine below)</td>
          </tr>
          <tr>
            <td>USB-A to USB-B wire</td>
            <td>1</td>
            <td>To connect the Arduino to the Raspberry Pi.</td>
          </tr>
          <tr>
            <td>Raspberry Pi 5</td>
            <td>1</td>
            <td>Above Pi 3, with enough computing power to support inference.</td>
          </tr>
          <tr>
            <td>High Quality Camera</td>
            <td>1</td>
            <td>The lens is sold usually separately.</td>
          </tr>
          <tr>
            <td>6mm Wide Angle Lens for the High Quality Camera</td>
            <td>1</td>
            <td>Any wide angle lens compatible with the High Quality Camera will do. Wide angle lenses are ideal for this case as a wide shot at short distances is required.</td>
          </tr>
          <tr>
            <td>Standard-mini Raspberry Pi Camera Cable</td>
            <td>1</td>
            <td>Only for use with the Raspberry Pi 5. For other Pi models use the standard-standard camera cable, which is usually supplied with the camera.</td>
          </tr>
          <tr>
            <td>Jumper wire</td>
            <td>As needed</td>
            <td></td>
          </tr>
          <tr>
            <td>PLA/PETG filament</td>
            <td>As needed</td>
            <td>For 3D printing</td>
          </tr>
          <tr>
            <td>M4 machine screws/bolts</td>
            <td>As needed</td>
            <td></td>
          </tr>
          <tr>
            <td>M2.5 machine screws/bolts/inserts</td>
            <td>As needed, 8 advised</td>
            <td>To fasten the Arduino and the Raspberry Pi to 3D printed surfaces.</td>
          </tr>
        </tbody>
      </table>
    </div>
    {{ subsubheading("Methods") }}
    <div>
      <p>For further information on developing the components, circuitry, and machine learning model, <b>in the sidebar</b>, refer to:</p>
      <ul>
        <li>Model for the machine learning model.</li>
        <li>CAD and circuitry for the CAD work and circuitry;</li>
      </ul>
      <p>This section mainly deals with integrating the CAD work, circuitry, and the machine learning model, and assumes that all the prerequisites in the aforementioned individual pages are met. The scripts supplied in the aforementioned two pages already enable serial communication between the Raspberry Pi and the Arduino.
      {{img(new_cdn("hardware/colony-counter-fsm.png"), width="80%")}}
      Figure 1. Finite state machine of Arduino-Raspberry Pi system.
      
      <p>When initialized, the device will be in a state ready to accept a Petri dish with colonies.</p>
      
    </div>
    {{ subsubheading("Testing, results, and next steps") }}
    <div>
      <p>Unfortunately, we were not able to perform integrative testing with the hardware components and the machine learning model in an actual in-lab setting. Testing was done component-wise, as follows:</p>
      
      <i>Machine learning model</i>
      <p>Our team obtained a mAP of 0.038 and a validation loss of 1.704 with the latest model generated. 50 epochs of training were used on ~5% of the entire AGAR {{cite('Majchrowska_Pawlowski_Gula_Bonus_Hanas_Loch_Pawlak_Roszkowiak_Golan_Drulis-Kawa_2021')}}; dataset to speed up training given limited resources. Qualitatively, there are situations where the model is extremely successful:</p>
      {{img(new_cdn("hardware/image.webp"), width="40%")}}
      Figure 1. Highly successful colony labelling with virtually all colonies accounted for.
      <p>There are also situations where the model is extremely underperforming:</p>
      {{img(new_cdn("hardware/image-1.webp"), width="40%")}}
      Figure 2. Pathological case of colony counting with colonies not detected.
      <p>And, in the average case, the model has an average performance, producing a colony count that is directionally correct but more often than not undercounting:</p>
      {{img(new_cdn("hardware/image-2.webp"), width="40%")}}
      Figure 3. Average colony counting case with directionally correct counting but missing detection.
      <p>This suggests to the team that in the limited time the team had to develop the model, the model is overfitting on the training dataset. In any future step, the team shall attempt to:</p>
      <ul>
        <li>Increase the size of the training and validation datasets;</li>
        <li>Increase the number of epochs for training;</li>
        <li>Increase the degree of variability in the training dataset, such as by rotating images;</li>
        <li>Identify specific pathological cases (such as in the second image in this sequence), determine the root cause behind those cases, and mitigate their impact on training.</li>
      </ul>
      
      <i>Physical hardware</i>
      <p>The physical hardware component does not need substantial testing as the product system is limited to a single device. Functionality of the physical hardware component, with the finite state machine framework, was trivially shown in a typical testing/debugging test scenario.</p>
      
      <i>Integrative testing</i>
      <p>Integrative testing was unfortunately not finished by the team, though the plan was to perform testing in two stages:</p>
      
      <i>Functionality testing</i>
      <p>This testing aims to ensure the model's functionality with the physical hardware component. The model would be loaded onto the script in the Raspberry Pi and the whole product system would be tested. Emphasis is laid on where the physical hardware interacts with the model:</p>
      <ul>
        <li>The transmission of the photograph taken by the camera to the model;</li>
        <li>The transmission of a resulting colony count to the Arduino microcontroller.</li>
      </ul>
      
      <i>End user testing</i>
      <p>This testing aims to ensure the product system's compatibility with the wet lab team's practices. The whole product system would be provided to the wet lab team and any feedback collected. Emphasis would be laid on any points that the hardware team could not provide:</p>
      <ul>
        <li>Performance in a typical laboratory environment</li>
        <li>Ease of use</li>
        <li>Additional features relevant to improving wet lab's workflow, tying into principles discussed in our process engineering improvements.</li>
      </ul>
      
    </div>
    {{ subsubheading("Making your own modifications") }}
    <div> 
      <p>Modularity is a key feature of our designs and for this specific project, damaged or customizable components may be swapped in or out without any loss to functionality provided they meet all the prerequisites above. The following restrictions apply:</p>
      <ul>
        <li>Most Arduino boards are 5V boards (only the small ones are 3.3V), while all Raspberry Pi boards are 3.3V. Note: <b>Do not connect the Arduino to the Raspberry Pi directly as this will damage the board. Use a USB connector or a logic level converter.</b></li>
        <li>To integrate more sensors with this product system, integrate them with the Arduino instead of the Raspberry Pi. If you need more I2C pins, use a multiplexer coming out of the SDA/SCK pins. Alternatively, use a wireless connection.</li>
        <li>To link this system to a GUI (graphical user interface), use the Raspberry Pi connect service and run the GUI on the Raspberry Pi computer itself. If the GUI needs to be run on a laptop computer, use a USB connection to the Raspberry Pi or use MQTT. Libraries such as tkinter or PyQT provide an easily accessible means of creating GUI windows on Python or C++.</li>
      </ul>
    </div>
  </div>
  {{subheading("Model")}}
  <div>
    {{subsubheading("Abstract")}}
    <div>
      <p>The colony counter project utilizes a machine learning model, specifically a fine-tuned version of the Detection Transformer (DETR) model published by Nicolas Carion et al. {{cite('Carion_Massa_Synnaeve_Usunier_Kirillov_Zagoruyko_2020')}}, to automate the counting of bacterial or fungal colonies on petri dishes. The team utilized the Annotated Germs for Automated Recognition (AGAR) dataset that contains more than 18000 photos of different bacteria with labels of whether the petri dish was countable, the location of each of the bacteria, and the total numbers of them {{cite('Majchrowska_Pawlowski_Gula_Bonus_Hanas_Loch_Pawlak_Roszkowiak_Golan_Drulis-Kawa_2021')}}. The goal of this machine learning model is to eliminate the need for manual counting, streamlining laboratory workflows and improving measurement efficiency, which includes: </p>
      <ol type="1" start="1">
        <li> Using the photo taken to accurately locate the location of each colony like objects </li>
        <li> Counting the number of such objects </li>
        <li>Returning to the user the number of colonies available. </li>
      </ol>
      <p>The model is currently under construction as the team is still in progress of finalizing the training, and the next step of assessing the model accuracy is to acquire photos from Wet Lab to test it on actual data collected the team. </p>
    </div>
    {{subsubheading("Introduction")}}
    <div>
      <p>Manual colony counting on petri dishes is time-consuming and error-prone. The colony counter project seeks to automate this process using machine learning, specifically by applying object detection techniques to colony identification. Leveraging the DETR model, the system is designed to detect bacterial or fungal colonies in petri dish images and provide an accurate count, streamlining laboratory operations and reducing human error.</p>
    </div>
    {{subsubheading("Prerequisites")}}
    <div>
      <p>The machine learning system requires several components:</p>
      <ul>
        <li><b>Dataset:</b> The AGAR dataset for training, which contains annotated petri dish images. {{cite('Majchrowska_Pawlowski_Gula_Bonus_Hanas_Loch_Pawlak_Roszkowiak_Golan_Drulis-Kawa_2021')}}</li>
        <li><b>Software:</b> The DETR model implemented using PyTorch, as well as standard machine learning libraries for training and inference. The following Python libraries are required: </li>
        <ul>
          <li><code>torch</code> and <code>torchvision</code>: For building and running the DETR model/code </li>
          <li><code>matplotlib</code>: For visualizing results</li>
          <li><code>numpy</code>: For numerical computations</li>
          <li><code>pandas</code>: For data manipulation</li>
          <li><code>scikit-learn</code>: For evaluation metrics like mAP</li>
          <li><code>cython</code>: For compiling C extensions for faster computation</li>
          <li><code>albumentations</code>: For data augmentation</li>
          <li><code>transformers</code>: For using transformer models</li>
          <li><code>timm</code>: PyTorch Image Models for working with pre-trained models</li>
          <li><code>monai</code>: For medical imaging deep learning, which can be used for image processing and transformations</li>
          <li><code>imantics</code>: For handling COCO-style annotations</li>
          <li><code>pycocotools</code>: For working with the COCO dataset format and evaluation metrics</li>
        </ul>
      </ul>
    </div>
    {{subsubheading("Product System")}}
    <div>
      <p>The core component of this project is a fine-tuned DETR model, which is trained to detect and count bacterial colonies on petri dishes. Consult the full scripts here: &lt;link to github script&gt;</p>
      <i>Machine learning model</i>
      <ul>
        <li><b>DETR Architecture: </b>The Detection Transformer (DETR) is a transformer-based model for object detection, introduced by Carion et al . DETR consists of two main components: a CNN backbone and a transformer-based encoder-decoder structure. The CNN backbone (ResNet-50) extracts feature maps from the input image, which are then passed through the transformer. The transformer encoder has 6 multiple layers of self-attention and feed-forward layers, allowing it to model global dependencies between image features. The decoder part has a similar structure, using learned object queries to predict bounding boxes and class labels for the detected objects. For this colony counter project, the CNN backbone was frozen during fine-tuning, meaning its weights were not updated to save on computational resources and avoid overfitting on limited data. Only the transformer layers and the final detection heads were fine-tuned to learn the specific task of bacterial colony detection and counting.</li>
        <li><b>Training Dataset:</b> The model is trained on the AGAR dataset, which contains over 18,000 images of petri dishes, annotated with colony locations and counts. This large dataset provides a robust foundation for training the model to detect colonies in a variety of conditions.</li>
      </ul>
      <i>Training and Performance</i>
      <ul>
        <li><b>Current Status:</b> The model is still undergoing training, with early results showing that it can detect colonies with moderate accuracy (for up to 300 colonies per image). Training has been limited to 50 epochs on approximately 5% of the AGAR dataset due to resource constraints. </li>
        <li><b>Challenges:</b> The model is currently exhibiting signs of overfitting, meaning it performs well on the training set but struggles to generalize to new data. Future improvements will focus on expanding the training dataset and increasing variability to improve generalization.</li>
      </ul>
      <i>Inference Process</i>
      <p>Once trained, the DETR model will be used to perform inference on new petri dish images. The process involves:</p>
      <ol type="1" start="1">
        <li><b>Input Image:</b> A high-resolution image of the petri dish is provided to the model.</li>
        <li><b>Detection:</b> The model identifies bacterial or fungal colonies in the image, treating each colony as an object.</li>
        <li><b>Counting:</b> The total number of detected colonies is counted, and the result is returned to the user or the script at large.</li>
      </ol>
    </div>
    {{subsubheading("Methods")}}
    <div>
      <i>Model Training</i>
      <p>The DETR model is fine-tuned on the AGAR dataset using supervised learning techniques. The dataset provides labeled images with colony locations and counts, allowing the model to learn to detect and count colonies effectively.</p>
      <ul>
        <li><b>Data Preprocessing:</b> Images from the AGAR dataset are first preprocessed, including resizing to a uniform scale. The preprocessing pipeline includes converting images to a format suitable for the transformer-based DETR architecture, where a fixed-size input is essential for consistent object detection results. </li>
        <li><b>Data Augmentation:</b> To improve model generalization, data augmentation techniques such as image rotation and scaling are applied to the training set that was taken from the portion of the AGAR dataset utilized. Augmentation ensures that the model can robustly detect colonies in diverse settings and angles, as colony appearance may vary slightly due to experimental setup variations.</li>
        <li><b>Training Configuration:</b> The fine-tuning of the DETR model is performed with a limited portion of the AGAR dataset. The training process involves optimizing both the detection and counting tasks by leveraging a combination of loss functions:
          <ul>
            <li><b>Bounding Box Regression Loss: </b>This helps the model learn the exact location and size of the colonies on the petri dishes.</li>
            <li><b>Classification Loss: </b>This component ensures that the model correctly identifies colonies as distinct objects from the background.</li>
          </ul>
        </li>
        <li><b>Batching &amp; Gradient Accumulation:</b> During training, batch sizes are configured to fit within the memory limits of the available GPU using ComputeCanada. Gradient accumulation is applied over several batches when the batch size is limited by memory, allowing the model to effectively train on larger datasets despite hardware limitations.</li>
        <li><b>Loss Function:</b> The model uses a combination of bounding box regression and classification loss to learn the location of the colonies.</li>
        <li><b>Validation: </b>A portion of the AGAR dataset is set aside for validation to track the model&#x27;s performance on unseen data during training. Validation metrics like mean Average Precision (mAP) and validation loss are used to evaluate the accuracy of the model in detecting and counting colonies.</li>
      </ul>
      <i>Model Evaluation</i>
      <ul>
        <li><b>Validation Set:</b> A portion of the AGAR dataset is held out for validation, allowing the team to monitor the model's performance during training. The primary evaluation metric is mAP, which measures the accuracy of the model's object detection capabilities.</li>
        <li><b>Challenges:</b> Current validation results show a mAP of 0.038 and a validation loss of 1.704, indicating room for improvement in both accuracy and generalization. Increasing the dataset size and improving data variability are key future steps.</li>
      </ul>
      <i>Testing and Next Steps</i>
      <p>The model is currently in the final stages of training. Once training is complete, the following steps will be taken:</p>
      <ul>
        <li><b>Real-World Testing:</b> The model will be tested on actual petri dish images from our wet lab team to assess its performance on data that is not processed in AGAR but actual data that it could be facing in daily operations. </li>
        <li><b>Refinement:</b> Based on the test results, the model will be fine-tuned further, with a focus on improving generalization and reducing overfitting. </li>
        <li><b>Future Improvements:</b> Potential improvements include increasing the training dataset size, applying more advanced augmentation techniques, and addressing specific cases where the model underperforms.</li>
      </ul>
    </div>
  </div>
  {{subheading('CAD and circuitry')}}
  <div>
    {{subsubheading('Abstract')}}
    <div>
      <p>A colony counter stand was built to accommodate the colony counter by supporting the electronics assemblies and provide a consistent environment for pictures of colonies to be taken with. The colony counter stand was fabricated with similar methods to the rack lab manager, including similar 3D printing settings and fastenings. The Arduino and the Raspberry Pi used in the assembly operate as a finite state machine to ease usage.</p>
    </div>
    {{subsubheading('Introduction')}}
    <div>
      <h2>Introduction</h2><p>Naturally, where there is a colony counter, there is a need to obtain pictures for the counter. Although this is fulfilled by our use of the camera, there is still a need to take high quality pictures consistently under even lighting, as well as to provide a sufficiently secure support for any electronics. We designed a stand for the colony counter in a similar vein to the &lt;link: lab management rack&gt; to allow secure placement of devices.</p>
    </div>
    {{subsubheading('Prerequisites')}}
    <div>
      <ul>
        <li>CAD software, prior CAD knowlege, and a 3D printer.</li>
        <li>An Arduino board, and wiring knowledge.</li>
        <li>Some Python knowledge, including use of the pyserial library.</li>
      </ul>
    </div>
    {{subsubheading('Product System')}}
    <div>
      {{img(new_cdn('hardware/colony-counter-wiring.png'), width="80%")}}
      <p>
        Figure 1. Wiring diagram for the colony counter Arduino.
      </p>
      {{img(new_cdn('hardware/colony-counter-fsm.png'), width="80%")}}
      <p>The user interaction workflow can be summed up with the diagram above. Besides slotting the petri dish into the baseplate, the only user interaction is with the two buttons on the attached breadboard. The first button controls when to activate the DETR model and if to save the model output while the second button controls if the output should be discarded.</p>
      <p><i>CAD, design considerations</i></p>
      <p>To ensure modularity, the stands of this colony counter stand are identical to the lab management solution rack, and use similar fastenings. Refer to our lab management page for details, under CAD and circuitry.</p>
      <p><i>CAD, final drawings</i></p>
      <p>
        The top plate was specially designed to accommodate multiple pieces of electronics, including the camera and all single board computers. Here is a visualisation. Please consult our lab management page, under CAD and circuitry, for visualisation of the vertical supports used.
      </p>
        <model-viewer
        class="w-full min-h-96 my-8"
        src="{{ url_for('static', filename = 'models/top-plate.gltf') }}"
        alt="Top colony counter plate"
        camera-controls
        touch-action="pan-y"></model-viewer>  
    </div>
    {{subsubheading('Methods')}}
    <div>
      <p>As the colony counter stand and the lab management rack were fabricated with similar conditions, consult our lab management page, under CAD and circuitry, for a full description.</p>
    </div>
  </div>
</div>
{% endblock %}